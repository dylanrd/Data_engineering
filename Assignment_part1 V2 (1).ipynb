{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "A very important aspect of supervised and semi-supervised machine learning is the quality of the labels produced by human labelers. Unfortunately, humans are not perfect and in some cases may even maliciously label things incorrectly. In this assignment, you will evaluate the impact of incorrect labels on a number of different classifiers.\n",
    "\n",
    "We have provided a number of code snippets you can use during this assignment. Feel free to modify them or replace them.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "The dataset you will be using is the [Adult Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). This dataset was created by Ronny Kohavi and Barry Becker and was used to predict whether a person's income is more/less than 50k USD based on census data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Start by loading and preprocessing the data. Remove NaN values, convert strings to categorical variables and encode the target variable (the string <=50K, >50K in column index 14)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.621381Z",
     "start_time": "2024-09-28T13:06:49.215383Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.794251Z",
     "start_time": "2024-09-28T13:06:53.624382Z"
    }
   },
   "source": [
    "# This can be used to load the dataset\n",
    "data = pd.read_csv(\"adult.csv\", header=0, na_values='?')\n",
    "data.head()\n",
    "\n",
    "# Basic data removal: NaN values, \"?\" values and also empty cells\n",
    "df = data.dropna()\n",
    "df = df[(df != \"\").all(axis=1)]\n",
    "df = df[(df != \"?\").all(axis=1)]\n",
    "# Map salary to 1 if salary is larger than 50k, else to 0\n",
    "df['salary'] = df['salary'].map({'<=50K': 0, '<=50K.': 0, '>50K': 1, '>50K.': 1})"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.917998Z",
     "start_time": "2024-09-28T13:06:53.795730Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "source code string cannot contain null bytes",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Visualising distribution of: age, fnlwgt, race, sex, capital-gain, capital-loss, hours-per-week, native-country, salary, to allow for a\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# better decision on which to keep/remove\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[0;32m      6\u001B[0m categorial_cols \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrace\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msex\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnative-country\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mValueError\u001B[0m: source code string cannot contain null bytes"
     ]
    }
   ],
   "source": [
    "# Visualising distribution of: age, fnlwgt, race, sex, capital-gain, capital-loss, hours-per-week, native-country, salary, to allow for a\n",
    "# better decision on which to keep/remove\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "categorial_cols = ['race', 'sex',  'native-country']\n",
    "numerical_cols = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# Loop through each column and create count plots for categorical data\n",
    "for col in categorial_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=df, x=col, palette=\"Set2\")\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "for col in numerical_cols:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[col], kde=True, bins=40, color=\"skyblue\")\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.920922Z",
     "start_time": "2024-09-28T13:06:53.919911Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# For race, and sex its worth to see correlation with salary to decide whether to keep or not. Because these data are usually bias inducing.\n",
    "# Also, worth doing chi-square test on these features.\n",
    "# For capital-gain and capital-loss also checking to see if there's correlation. Because there's not many people in the dataset that have values for this.\n",
    "\n",
    "def contingency_chi_square_tests(col1, col2, data, visualise=True):\n",
    "    contingency = pd.crosstab(data[col1], data[col2])\n",
    "    chi2_value, p_value, _, _ = chi2_contingency(contingency)\n",
    "    print(f\"Chi-square statistic for {col1}: {chi2_value}, p-value: {p_value}\")\n",
    "\n",
    "    if visualise:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        sns.countplot(data=df, x=col1, hue=col2, palette='Set1')\n",
    "        plt.title('Count of Salary Above Threshold by ' + col1)\n",
    "        plt.xlabel(col1)\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend(title='Salary Above Threshold')\n",
    "        plt.show()\n",
    "\n",
    "# Contingency tables for the four features we're checking\n",
    "contingency_chi_square_tests('race', 'salary', df)\n",
    "contingency_chi_square_tests('sex', 'salary', df)\n",
    "contingency_chi_square_tests('capital-gain', 'salary', df, visualise=False)\n",
    "contingency_chi_square_tests('capital-loss', 'salary', df, visualise=False)\n",
    "\n",
    "# Doing correlation tests of these four features. First need to convert race and sex into numerical though. Then print results.\n",
    "race_label_enc = LabelEncoder()\n",
    "sex_label_enc = LabelEncoder()\n",
    "df['race_encoded'] = race_label_enc.fit_transform(df['race'])\n",
    "df['sex_encoded'] = sex_label_enc.fit_transform(df['sex'])\n",
    "\n",
    "correlation_race = df['race_encoded'].corr(df['salary'])\n",
    "correlation_sex = df['sex_encoded'].corr(df['salary'])\n",
    "correlation_capital_gain = df['capital-gain'].corr(df['salary'])\n",
    "correlation_capital_loss = df['capital-loss'].corr(df['salary'])\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(f\"Correlation between race and salary: {correlation_race}\")\n",
    "print(f\"Correlation between sex and salary: {correlation_sex}\")\n",
    "print(f\"Correlation between capital-gain and salary: {correlation_capital_gain}\")\n",
    "print(f\"Correlation between capital-loss and salary: {correlation_capital_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data classification\n",
    "Choose at least 4 different classifiers and evaluate their performance in predicting the target variable. \n",
    "\n",
    "#### Preprocessing\n",
    "Think about how you are going to encode the categorical variables, normalization, whether you want to use all of the features, feature dimensionality reduction, etc. Justify your choices \n",
    "\n",
    "A good method to apply preprocessing steps is using a Pipeline. Read more about this [here](https://machinelearningmastery.com/columntransformer-for-numerical-and-categorical-data/) and [here](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf). \n",
    "\n",
    "<!-- #### Data visualization\n",
    "Calculate the correlation between different features, including the target variable. Visualize the correlations in a heatmap. A good example of how to do this can be found [here](https://towardsdatascience.com/better-heatmaps-and-correlation-matrix-plots-in-python-41445d0f2bec). \n",
    "\n",
    "Select a features you think will be an important predictor of the target variable and one which is not important. Explain your answers. -->\n",
    "\n",
    "#### Evaluation\n",
    "Use a validation technique from the previous lecture to evaluate the performance of the model. Explain and justify which metrics you used to compare the different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.921951Z",
     "start_time": "2024-09-28T13:06:53.921951Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Fresh load of data and repeating cleanup from before\n",
    "data = pd.read_csv(\"adult.csv\", header=0, na_values='?')\n",
    "df = data.dropna()\n",
    "df = df[(df != \"\").all(axis=1)]\n",
    "df = df[(df != \"?\").all(axis=1)]\n",
    "df['salary'] = df['salary'].map({'<=50K': 0, '<=50K.': 0, '>50K': 1, '>50K.': 1})\n",
    "\n",
    "# From data exploration the final decisions for each column are as follows:\n",
    "# 1. Remove: education (redundant as education-num exists), race (based on chi-square and correlation tests above), native-country (most data is US based, and if we one-hot-encode it, unnecessarily too many columns will be added)\n",
    "# 2. Normalise: age, fnlwgt, capital-gain, capital-loss, hours-per-week\n",
    "# 3. One-Hot-Encode: workclass, marital-status, occupation, relationship, sex. Workclass could be Label Encoded but that would be subjective.\n",
    "\n",
    "columns_to_remove = ['education', 'race', 'native-country']\n",
    "columns_to_one_hot = ['workclass', 'marital-status', 'occupation', 'relationship', 'sex']\n",
    "columns_to_normalize = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "# df = pd.get_dummies(df, columns=columns_to_one_hot, drop_first=True)\n",
    "# df = df.drop(columns=columns_to_remove)\n",
    "# scaler = StandardScaler()\n",
    "# df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
    "# df.to_csv(\"processed_data.csv\", index=False)\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('remove', 'drop', columns_to_remove),\n",
    "        ('onehot', OneHotEncoder(), columns_to_one_hot),\n",
    "        ('normalize', StandardScaler(), columns_to_normalize)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-28T13:06:53.923111Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# Apply your model to data\n",
    "def apply_models(X, y, models, model_names, ct):\n",
    "    # Split data into X and y and get train/test split (75/25)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "    results = {}\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Define pipeline\n",
    "        pipeline = Pipeline(steps=[(\"preprocessor\", ct), (\"classifier\", model)])\n",
    "        # Fit model on training data\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        # Evaluate fit\n",
    "        y_predicted = pipeline.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_predicted)\n",
    "        f1_value = f1_score(y_test, y_predicted)\n",
    "        report = classification_report(y_test, y_predicted, output_dict=True)\n",
    "\n",
    "        results[model_names[i]] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_score\": f1_value,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.924154Z",
     "start_time": "2024-09-28T13:06:53.924154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup data\n",
    "y = df['salary']\n",
    "X = df.drop(columns=['salary'])\n",
    "\n",
    "# Initialise models\n",
    "random_forest = RandomForestClassifier()\n",
    "log_regr = LogisticRegression(max_iter=10000)\n",
    "svm = LinearSVC(dual=False, max_iter=10000)\n",
    "grad_boost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "\n",
    "# Call method to train and evaluate\n",
    "models = [random_forest, log_regr, svm, grad_boost]\n",
    "model_names = [\"random_forest\", \"logistic_regression\", \"SVM\", \"Gradient Booster\"]\n",
    "results = apply_models(X, y, models, model_names, ct)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label perturbation\n",
    "To evaluate the impact of faulty labels in a dataset, we will introduce some errors in the labels of our data.\n",
    "\n",
    "\n",
    "#### Preparation\n",
    "Start by creating a method which alters a dataset by selecting a percentage of rows randomly and swaps labels from a 0->1 and 1->0. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.924658Z",
     "start_time": "2024-09-28T13:06:53.924658Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Given a label vector, create a new copy where a random fraction of the labels have been flipped.\"\"\"\n",
    "def perturbate(y: np.ndarray, fraction: float) -> np.ndarray:\n",
    "    if fraction == 0:\n",
    "        return y\n",
    "\n",
    "    # Create a copy of the original label array\n",
    "    copy = y.copy()\n",
    "\n",
    "    # Calculate the number of labels to flip\n",
    "    n_flip = int(fraction * len(copy))\n",
    "\n",
    "    # Randomly select indices to flip\n",
    "    flip_indices = np.random.choice(len(copy), size=n_flip, replace=False)\n",
    "\n",
    "    # Flip the selected labels (0 -> 1, 1 -> 0)\n",
    "    copy[flip_indices] = 1 - copy[flip_indices]\n",
    "\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "Create a number of new datasets with perturbed labels, for fractions ranging from `0` to `0.5` in increments of `0.1`.\n",
    "\n",
    "Perform the same experiment you did before, which compared the performances of different models except with the new datasets. Repeat your experiment at least 5x for each model and perturbation level and calculate the mean and variance of the scores. Visualize the change in score for different perturbation levels for all of the models in a single plot. \n",
    "\n",
    "State your observations. Is there a change in the performance of the models? Are there some classifiers which are impacted more/less than other classifiers and why is this the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-28T13:06:53.925880Z",
     "start_time": "2024-09-28T13:06:53.925880Z"
    }
   },
   "outputs": [],
   "source": [
    "perturbed_label_sets = []\n",
    "for i in [0, 0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    perturbed_label_sets.append(perturbate(y.to_numpy(), i))\n",
    "\n",
    "perturbed_label_sets = np.array(perturbed_label_sets)\n",
    "\n",
    "# Run our experiment 6 times for each level\n",
    "experiment_results = []\n",
    "for i in range(6):\n",
    "    perturbed_y = perturbed_label_sets[i]\n",
    "    results = []\n",
    "    for j in range(5):\n",
    "        res = apply_models(X, perturbed_y, models, model_names, ct)\n",
    "        results.append(res)\n",
    "    experiment_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict = {}\n",
    "perturbation_names = [\"0.0\", \"0.1\", \"0.2\", \"0.3\", \"0.4\" \"0.5\"]\n",
    "for model_name in model_names:\n",
    "    dict[model_name] = {}\n",
    "\n",
    "\n",
    "for name_idx, current_perturbation in enumerate(experiment_results):\n",
    "    current_perturbation_name = perturbation_names[name_idx]\n",
    "\n",
    "    # In this current_perturbation array I have 5 dictionaries, each dictionary is the 'result' dictionary from the apply_models methods\n",
    "    # Loop over each of the 4 model names\n",
    "    for model_name in model_names:\n",
    "        accuracies = []\n",
    "        f1_scores = []\n",
    "\n",
    "        # Across the 5 experiments done for this perturbation, find the accuracy and f1 score\n",
    "        for i in range(5):\n",
    "            accuracies.append(current_perturbation[i][model_name][\"accuracy\"])\n",
    "            f1_scores.append(current_perturbation[i][model_name][\"f1_score\"])\n",
    "\n",
    "        # Compute the means and variances and add them to our dict\n",
    "        accuracies = np.array(accuracies)\n",
    "        mean_acc = np.mean(accuracies)\n",
    "        var_acc = np.var(accuracies)\n",
    "\n",
    "        f1_scores = np.array(f1_scores)\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "        var_f1 = np.var(f1_scores)\n",
    "\n",
    "        dict[model_name][current_perturbation_name] = {}\n",
    "        dict[model_name][current_perturbation_name][\"mean_acc\"] = mean_acc\n",
    "        dict[model_name][current_perturbation_name][\"var_acc\"] = var_acc\n",
    "        dict[model_name][current_perturbation_name][\"mean_f1\"] = mean_f1\n",
    "        dict[model_name][current_perturbation_name][\"var_f1\"] = var_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-28T13:06:53.926940Z"
    }
   },
   "outputs": [],
   "source": [
    "print(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-09-28T13:06:53.927945Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the above dict or just copy it from the cell output\n",
    "data = {\n",
    "    'random_forest': {\n",
    "        '0.0': {'mean_acc': 0.850663364585176, 'var_acc': 1.0779058435508168e-05, 'mean_f1': 0.6745150622371725, 'var_f1': 4.2786319678102166e-05},\n",
    "        '0.1': {'mean_acc': 0.7726339996462055, 'var_acc': 1.9607960331212577e-06, 'mean_f1': 0.5600180987145531, 'var_f1': 1.2116287255154797e-05},\n",
    "        '0.2': {'mean_acc': 0.6880771271891032, 'var_acc': 1.7511354295158144e-06, 'mean_f1': 0.4725795833009965, 'var_f1': 3.0515977652873996e-05},\n",
    "        '0.3': {'mean_acc': 0.6051477091809658, 'var_acc': 4.9924257760000484e-06, 'mean_f1': 0.42028628830287856, 'var_f1': 2.9855172016711396e-05},\n",
    "        '0.4': {'mean_acc': 0.5319830178666195, 'var_acc': 1.3648592368435312e-05, 'mean_f1': 0.42783124334751416, 'var_f1': 2.4096517115953002e-05},\n",
    "        '0.5': {'mean_acc': 0.49752343888200956, 'var_acc': 3.8552516961464565e-06, 'mean_f1': 0.49609973433320376, 'var_f1': 9.045271080261405e-06}\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        '0.0': {'mean_acc': 0.8482221829117282, 'var_acc': 5.404236752633861e-06, 'mean_f1': 0.6642259399929304, 'var_f1': 1.5376774131171858e-05},\n",
    "        '0.1': {'mean_acc': 0.7765257385459048, 'var_acc': 7.743673577340858e-06, 'mean_f1': 0.5420379855865222, 'var_f1': 5.07118701356174e-05},\n",
    "        '0.2': {'mean_acc': 0.7013797983371661, 'var_acc': 2.6642542971583513e-06, 'mean_f1': 0.44570892787965677, 'var_f1': 3.397682390563077e-05},\n",
    "        '0.3': {'mean_acc': 0.6336104723155846, 'var_acc': 3.850870728309899e-06, 'mean_f1': 0.3685328314735121, 'var_f1': 1.8926606908129507e-05},\n",
    "        '0.4': {'mean_acc': 0.5655934901822042, 'var_acc': 6.711016873015794e-06, 'mean_f1': 0.3262344101413093, 'var_f1': 2.768501451824968e-05},\n",
    "        '0.5': {'mean_acc': 0.49973465416592966, 'var_acc': 4.365321522828163e-06, 'mean_f1': 0.5336554420663222, 'var_f1': 4.86149606694237e-05}\n",
    "    },\n",
    "    'SVM': {\n",
    "        '0.0': {'mean_acc': 0.8486821156907837, 'var_acc': 6.12834815075749e-06, 'mean_f1': 0.6620178649921415, 'var_f1': 1.8178421463570093e-05},\n",
    "        '0.1': {'mean_acc': 0.7728285865911906, 'var_acc': 3.875904830233002e-06, 'mean_f1': 0.5239848918303407, 'var_f1': 3.77272335514664e-05},\n",
    "        '0.2': {'mean_acc': 0.6992393419423315, 'var_acc': 2.5741315302354462e-06, 'mean_f1': 0.4357394699165799, 'var_f1': 4.469562335208212e-05},\n",
    "        '0.3': {'mean_acc': 0.6338227489828409, 'var_acc': 4.69076484782767e-06, 'mean_f1': 0.3666255879971217, 'var_f1': 1.347869948978629e-05},\n",
    "        '0.4': {'mean_acc': 0.5655581107376614, 'var_acc': 7.126582964938144e-06, 'mean_f1': 0.32540177487387817, 'var_f1': 2.8725461519248183e-05},\n",
    "        '0.5': {'mean_acc': 0.4992570316646029, 'var_acc': 5.3497875809512335e-06, 'mean_f1': 0.5332402866887701, 'var_f1': 4.4589763741300735e-05}\n",
    "    },\n",
    "    'Gradient Booster': {\n",
    "        '0.0': {'mean_acc': 0.8611003007252787, 'var_acc': 4.556394305755691e-05, 'mean_f1': 0.6890940267095218, 'var_f1': 0.0002909976855566734},\n",
    "        '0.1': {'mean_acc': 0.789739961082611, 'var_acc': 7.366284490851819e-07, 'mean_f1': 0.5619686926545665, 'var_f1': 6.9353572229064775e-06},\n",
    "        '0.2': {'mean_acc': 0.7120467008667963, 'var_acc': 8.350750548974236e-06, 'mean_f1': 0.4543389784176517, 'var_f1': 6.589167080630133e-05},\n",
    "        '0.3': {'mean_acc': 0.6410047762250134, 'var_acc': 3.3652091510031067e-06, 'mean_f1': 0.3701109062280361, 'var_f1': 1.806822759149987e-05},\n",
    "        '0.4': {'mean_acc': 0.56592959490536, 'var_acc': 2.8369896004272935e-06, 'mean_f1': 0.32731054242641305, 'var_f1': 3.9509730686928536e-05},\n",
    "        '0.5': {'mean_acc': 0.5005306916681408, 'var_acc': 1.6053117858142055e-06, 'mean_f1': 0.5309233631348643, 'var_f1': 3.0664687867147875e-05}\n",
    "    }\n",
    "}\n",
    "\n",
    "# x-values\n",
    "x_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Metrics to plot\n",
    "metrics = ['mean_acc', 'var_acc', 'mean_f1', 'var_f1']\n",
    "\n",
    "# Extracting y-values for each model and each metric\n",
    "for metric in metrics:\n",
    "    y_random_forest = [data['random_forest'][str(i)][metric] for i in x_values]\n",
    "    y_logistic_regression = [data['logistic_regression'][str(i)][metric] for i in x_values]\n",
    "    y_svm = [data['SVM'][str(i)][metric] for i in x_values]\n",
    "    y_gradient_booster = [data['Gradient Booster'][str(i)][metric] for i in x_values]\n",
    "\n",
    "    # Plotting the data for each metric\n",
    "    plt.figure()\n",
    "    plt.plot(x_values, y_random_forest, label='Random Forest', marker='o')\n",
    "    plt.plot(x_values, y_logistic_regression, label='Logistic Regression', marker='o')\n",
    "    plt.plot(x_values, y_svm, label='SVM', marker='o')\n",
    "    plt.plot(x_values, y_gradient_booster, label=\"Grad-Booster\", marker='o')\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.xlabel('Percent of y-labels perturbed (0.1%)')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f'Change in {metric} with percentage of perturbed y-labels across the four chosen models')\n",
    "\n",
    "    # Adding a legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observations + explanations: max. 400 words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Examining the first graph, which plots the mean accuracy across the five experiments, we observe that all four models start with high accuracy (~85%) when there are no faulty labels. As the percentage of perturbed labels increases, we see a roughly linear decrease in accuracy for all models. When half of the data is perturbed, all models reach a mean accuracy of about 50%. Notably, the random forest model appears more affected by the perturbations. This could be because some of its decision trees might receive more of the perturbed data, performing worse and consequently lowering the overall performance. \n",
    "\n",
    "Examining the third graph, which plots the mean f1-score across the five experiments, we see that as we increase the label noise from 0% to 40%, the F1 scores drop for all models in (approximate) linear fashion; because the relationship between features and the target label is getting increasingly corrupted. However, when reaching 50% perturbation, a sudden increase in F1 scores is seen across all models. A reason for this could be that the data is so randomized that models might essentially revert to treating both classes as near-equal probabilities. Because F1 is a harmonic mean of precision and recall, this class balance might lead to an improvement in the metric. The models may also be overfitting to the noise at this point. Additionally, as the distribution of label errors becomes highly symmetric or random, models like random forest that rely more on the structure of the input features, seem to maintain some ability to correctly classify parts of the data, and therefore are less affected by the perturbed labels. \n",
    "\n",
    "We see little to none variance in the experiments (second and fourth graph above), both the mean accuracies and the f1-scores have variance in order of 1e-5 magnitude. Suggesting that the seen metric scores and patterns are somewhat reliable across multiple attempts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion\n",
    "\n",
    "1)  Discuss how you could reduce the impact of wrongly labeled data or correct wrong labels. <br />\n",
    "    max. 400 words\n",
    "\n",
    "There are several approaches to deal with label noise. The first approach could be to integrate human review; having experts manually review and correct labels. This can be time-consuming but effective for smaller datasets. Another approach could be to smartly detect noisy labels. After training a model, samples where the model's confidence is very low or where predictions frequently mismatch the given labels could be flagged for review. When using ensemble models like the gradient booster, high quantity of disagreements between the models in the ensemble could indicate label perturbations.  After these potential perturbed labels are detected, they can be pruned or fixed. Additionally, unsupervised anomaly detection methods on the dataset can also be used to detect potentially mislabeled samples, as they might appear as outliers based on the feature values. \n",
    "\n",
    "The above approaches involve identifying and fixing the noisy labels or reducing their negative effects directly, however, another way is to make the model training process itself more robust. Examples include changing the used loss function. Mean absolute error could be used instead of cross-entropy loss, so that less importance is given to highly confident predictions. Early stopping could also be adopted to avoid overfitting to noise, for the same reasons regularization can be added. A risky but interesting approach could be bootstrap learning. After training the model and predicting on the training data, you combine the original and predicted labels to create new training labels. This approach could work well when base model is reasonably accurate and the label noise is not significantly high as was during our experiments.\n",
    "\n",
    "    Authors: Youri Arkesteijn, Tim van der Horst and Kevin Chong.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Workflow\n",
    "\n",
    "From part 1, you will have gone through the entire machine learning workflow which are they following steps:\n",
    "\n",
    "1) Data Loading\n",
    "2) Data Pre-processing\n",
    "3) Machine Learning Model Training\n",
    "4) Machine Learning Model Testing\n",
    "\n",
    "You can see these tasks are very sequential, and need to be done in a serial fashion. \n",
    "\n",
    "As a small perturbation in the actions performed in each of the steps may have a detrimental knock-on effect in the task that comes afterwards.\n",
    "\n",
    "In the final part of Part 1, you will have experienced the effects of performing perturbations to the machine learning model training aspect and the reaction of the machine learning model testing section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ## Part 2 Data Discovery\n",
    "\n",
    "Now we have a some datasets that are related to each other.\n",
    "\n",
    "**Altogether they are the same as the adult dataset used in the part 1 of the assignment.**\n",
    "\n",
    "In this scenario, one can see the utility of the subsets of data can impact the outcome of the task from the previous section.\n",
    "\n",
    "Because the data is split up, we want to be able to re-construct the data through data discovery.\n",
    "\n",
    "As data discovery will allow you to be able to find relations that can be used to reconstruct the entire dataset.\n",
    "\n",
    "Implement a method of your choice to perform the data discovery to be able to recover the entire dataset from part 1 of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from itertools import combinations\n",
    "data_01 = pd.read_csv(\"data_adult_discovery/data_adult_0_1.csv\", header=0, na_values='?')\n",
    "\n",
    "data_02 = pd.read_csv(\"data_adult_discovery/data_adult_0_2.csv\", header=0, na_values='?')\n",
    "\n",
    "data_03 = pd.read_csv(\"data_adult_discovery/data_adult_1_1.csv\", header=0, na_values='?')\n",
    "\n",
    "data_04 = pd.read_csv(\"data_adult_discovery/data_adult_1_2.csv\", header=0, na_values='?')\n",
    "\n",
    "data_05 = pd.read_csv(\"data_adult_discovery/data_adult_2_1.csv\", header=0, na_values='?')\n",
    "\n",
    "data_06 = pd.read_csv(\"data_adult_discovery/data_adult_2_2.csv\", header=0, na_values='?')\n",
    "\n",
    "datasets = [data_01, data_02, data_03, data_04, data_05, data_06]\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    c_inter = len(set1.intersection(set2))\n",
    "    c_union = len(set1.union(set2))\n",
    "    return c_inter / c_union\n",
    "\n",
    "def jaccard_similarity_on_column_names(dataset1, dataset2):\n",
    "    col_names1 = set(dataset1.columns)\n",
    "    col_names2 = set(dataset2.columns)\n",
    "    return jaccard_similarity(col_names1, col_names2)\n",
    "\n",
    "def max_jaccard_similarity_on_column_values(dataset1, dataset2):\n",
    "    max_jacc = 0\n",
    "    max_col1_name = ''\n",
    "    max_col2_name = ''\n",
    "    for column1_name in dataset1.columns:\n",
    "        for column2_name in dataset2.columns:\n",
    "            column1 = dataset1.loc[:,column1_name]\n",
    "            column2 = dataset2.loc[:,column2_name]\n",
    "            similarity = jaccard_similarity(set(column1), set(column2))\n",
    "            if similarity > max_jacc:\n",
    "                max_jacc = similarity\n",
    "                max_col1_name = column1_name\n",
    "                max_col2_name = column2_name\n",
    "\n",
    "    return max_jacc, max_col1_name, max_col2_name\n",
    "\n",
    "def set_containment(set1, set2):\n",
    "    c_inter = len(set1.intersection(set2))\n",
    "    c_set1 = len(set1)\n",
    "    return c_inter / c_set1\n",
    "\n",
    "def set_containment_on_column_names(dataset1, dataset2):\n",
    "    col_names1 = set(dataset1.columns)\n",
    "    col_names2 = set(dataset2.columns)\n",
    "    return set_containment(col_names1, col_names2)\n",
    "\n",
    "# Stronger indication for joinable columns, method currently doesn't return column names but is possible\n",
    "def max_set_containment_on_column_values(dataset1, dataset2):\n",
    "    max_setcon = 0\n",
    "    max_col1_name = ''\n",
    "    max_col2_name = ''\n",
    "    for column1_name in dataset1.columns:\n",
    "        for column2_name in dataset2.columns:\n",
    "            column1 = dataset1.loc[:,column1_name]\n",
    "            column2 = dataset2.loc[:,column2_name]\n",
    "            similarity = set_containment(set(column1), set(column2))\n",
    "            if similarity > max_setcon:\n",
    "                max_setcon = similarity\n",
    "                max_col1_name = column1_name\n",
    "                max_col2_name = column2_name\n",
    "\n",
    "    # Added this because set containment is order dependent, unsure if actually needed\n",
    "    for column1_name in dataset2.columns:\n",
    "        for column2_name in dataset1.columns:\n",
    "            column1 = dataset2.loc[:,column1_name]\n",
    "            column2 = dataset1.loc[:,column2_name]\n",
    "            similarity = set_containment(set(column1), set(column2))\n",
    "            if similarity > max_setcon:\n",
    "                max_setcon = similarity\n",
    "                max_col1_name = column1_name\n",
    "                max_col2_name = column2_name\n",
    "\n",
    "    return max_setcon, max_col1_name, max_col2_name\n",
    "\n",
    "# Not doing set overlap because apparently is less reliable\n",
    "\n",
    "\n",
    "def discovery_algorithm(datasets, similarity_threshold):\n",
    "    \"\"\"Function should be able to perform data discovery to find related datasets\n",
    "    Possible Input: List of datasets\n",
    "    Output: List of pairs of related datasets\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Get all dataset pairs\n",
    "    dataset_pairs = list(combinations(enumerate(datasets), 2))\n",
    "\n",
    "    for (index1, dataset1), (index2, dataset2) in dataset_pairs:\n",
    "\n",
    "        similarity, column_name_1, column_name_2 = max_set_containment_on_column_values(dataset1, dataset2)\n",
    "\n",
    "        if similarity >= similarity_threshold:\n",
    "            results.append((index1, index2, similarity, column_name_1, column_name_2))\n",
    "\n",
    "    return results\n",
    "\n",
    "result = discovery_algorithm(datasets, similarity_threshold=0.5)\n",
    "print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-28T13:06:53.930044Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "# Combine the related datasets\n",
    "def combineDatasets(datasets, related_pairs):\n",
    "    merged_datasets = []\n",
    "\n",
    "    for (index1, index2, common_columns, similarity_score) in related_pairs:\n",
    "        #Merging on the common columns\n",
    "        merged_data = pd.merge(datasets[index1], datasets[index2], on=list(common_columns), how='outer')\n",
    "        merged_data.fillna(0, inplace=True)\n",
    "\n",
    "        merged_datasets.append(merged_data)\n",
    "\n",
    "    return merged_datasets\n",
    "\n",
    "# merged_datasets = combineDatasets(datasets, result)\n",
    "\n",
    "# merged_datasets[5].to_csv('output.csv', index=False)\n",
    "# merged_datasets[2].head()\n",
    "\n",
    "\n",
    "# Selecting the testing data\n",
    "def sampleTestingDataset():\n",
    "    # This can be used to load the dataset\n",
    "    fullDatasets = pd.read_csv(\"adult.csv\", header=None, na_values='?')\n",
    "\n",
    "    test_df = fullDatasets.sample(frac=0.2)\n",
    "\n",
    "    return test_df\n",
    "\n",
    "# df = sampleTestingDataset()\n",
    "# df.head()\n",
    "\n",
    "# Combine the datasets through random combinations of the datasets\n",
    "def randomCombinations():\n",
    "\n",
    "    numbers = random.sample(range(6), 2)\n",
    "\n",
    "    dataset1 = datasets[numbers[0]]\n",
    "    dataset2 = datasets[numbers[1]]\n",
    "\n",
    "    col_names1 = set(dataset1.columns)\n",
    "    col_names2 = set(dataset2.columns)\n",
    "\n",
    "    common_columns = col_names1.intersection(col_names2)\n",
    "\n",
    "    merged_data = pd.merge(dataset1, dataset2, on=list(common_columns), how='outer')\n",
    "    merged_data.fillna(0, inplace=True)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "\n",
    "# def merge_All_Datasets(datasets):\n",
    "\n",
    "#     # Combine all datasets into one\n",
    "#     common_columns1 = datasets[0].columns.intersection(datasets[1].columns)\n",
    "#     merged_df1 = pd.merge(datasets[0], datasets[1], on=list(common_columns1), how='outer')\n",
    "#     merged_df1.fillna(0, inplace=True)\n",
    "\n",
    "#     common_columns2 = datasets[2].columns.intersection(datasets[3].columns)\n",
    "#     merged_df2 = pd.merge(datasets[2], datasets[3], on=list(common_columns2), how='outer')\n",
    "#     merged_df2.fillna(0, inplace=True)\n",
    "\n",
    "#     common_columns3 = datasets[4].columns.intersection(datasets[5].columns)\n",
    "#     merged_df3 = pd.merge(datasets[4], datasets[5], on=list(common_columns3), how='outer')\n",
    "#     merged_df3.fillna(0, inplace=True)\n",
    "\n",
    "#     frames = [merged_df1, merged_df2, merged_df3]\n",
    "\n",
    "#     all_data = pd.concat(frames)\n",
    "\n",
    "#     return all_data\n",
    "\n",
    "def remove_unrelated_columns(datasets):\n",
    "    # Remove the column which is less related\n",
    "    columns = datasets.columns\n",
    "    n = len(columns)\n",
    "\n",
    "    related_columns = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            col1 = datasets.iloc[:, i]\n",
    "            col2 = datasets.iloc[:, j]\n",
    "\n",
    "            similarity = set_containment(set(col1), set(col2))\n",
    "            if similarity > 0.5:\n",
    "                print(\"Similar column found: \"+ str(i) +\" \" + str(j))\n",
    "                if columns[i] not in related_columns:\n",
    "                    related_columns.append(columns[i])\n",
    "                if columns[j] not in related_columns:\n",
    "                    related_columns.append(columns[j])\n",
    "\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        if columns[i] not in related_columns:\n",
    "            datasets.drop(columns=columns[i], inplace=True)\n",
    "\n",
    "    return datasets\n",
    "\n",
    "def merge_datasets_on_column_value(datasets):\n",
    "    final_merged_ds = 0\n",
    "    processed = []\n",
    "    drop_list = []\n",
    "    disc_results = discovery_algorithm(datasets, similarity_threshold=0.5)\n",
    "    for (index1, index2, similarity, column_name_1, column_name_2) in disc_results:\n",
    "        if index1 not in processed and index2 not in processed:\n",
    "            if column_name_1 == column_name_2:\n",
    "                final_merged_ds = pd.merge(ds[index1], ds[index2], on=column_name_1, how='outer')\n",
    "            else:\n",
    "                final_merged_ds = pd.merge(ds[index1], ds[index2], left_on=column_name_1, right_on=column_name_2, how='outer')\n",
    "                drop_list.append(column_name_2)\n",
    "            processed.append(index1)\n",
    "            processed.append(index2)\n",
    "        elif index1 in processed and index2 in processed:\n",
    "            continue\n",
    "        elif index1 in processed:\n",
    "            if column_name_1 == column_name_2:\n",
    "                final_merged_ds = pd.merge(final_merged_ds, ds[index2], on=column_name_1, how='outer')\n",
    "            else:\n",
    "                final_merged_ds = pd.merge(final_merged_ds, ds[index2], left_on=column_name_1, right_on=column_name_2, how='outer')\n",
    "                drop_list.append(column_name_2)\n",
    "            processed.append(index2)\n",
    "        elif index2 in processed:\n",
    "            if column_name_1 == column_name_2:\n",
    "                final_merged_ds = pd.merge(ds[index1], final_merged_ds, on=column_name_1, how='outer')\n",
    "            else:\n",
    "                final_merged_ds = pd.merge(ds[index1], final_merged_ds, left_on=column_name_1, right_on=column_name_2, how='outer')\n",
    "                drop_list.append(column_name_1)\n",
    "            processed.append(index1)\n",
    "    for dropy in drop_list:\n",
    "        final_merged_ds.drop(dropy, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return final_merged_ds\n",
    "\n",
    "\n",
    "ds = [data_01,data_02,data_03,data_04,data_05,data_06]\n",
    "# ds_combined = merge_All_Datasets(ds)\n",
    "ds_combined = merge_datasets_on_column_value(ds)\n",
    "ds_combined.head()\n",
    "# newds = remove_unrelated_columns(ds_combined)\n",
    "# newds.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Do some testing using different degrees of relations on the downstream task.\n",
    "##\n",
    "\n",
    "#fullDatasets = pd.read_csv(\"adult.csv\", header=None, na_values='?')\n",
    "#merged_dataset.compare(fullDatasets)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Following the same workflow as Part 1 of the assignment, you will need to perform the steps once again.\n",
    "This means that with the difference in setting, there has now been a change in the data loading portion of the workflow. \n",
    "\n",
    "<!-- While performing data discovery, one can check how having more data of different relations can effect the outcome of the downstream task. -->\n",
    "\n",
    "As you perform the act of data discovery you will be piecing the data back together one by one by finding the relation between the datasets.\n",
    "\n",
    "As you piece the data back together, the entire dataset will be available in varying portions.\n",
    "\n",
    "Then using the different portions of available data, fit the models that were used previously, and examine the results when testing on the appropriate data from the testing samples.\n",
    "\n",
    "*The dataset from part 1 can be treated as the groundtruth, so you can try and random sample from that dataset to produce the testing samples that can be used in this part.* \n",
    "\n",
    "***\n",
    "\n",
    "One can also evaluate on the effect of incorrectness of data discovery.\n",
    "\n",
    "Suppose what you have done for data discovery is correct, this means the relations that are found are correct.\n",
    "\n",
    "What if you perform a perturbation on the relations between the files? \n",
    "\n",
    "This would mean the dataset will be incorrectly joined, do you think there will be an impact on the outcome of the model that will be trained and then tested on the groudtruth dataset?\n",
    "\n",
    "What would the effect be on the downstream tasks as mentioned in the machine learning workflow such as data pre-processing, machine learning model training and testing?\n",
    "\n",
    "This can be evaluated in the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_test_models(X_combined, y_combined, X_original, y_original, models, model_names, ct):\n",
    "    # Split data into X and y and get train/test split (75/25)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\n",
    "    results = {}\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        # Define pipeline\n",
    "        pipeline = Pipeline(steps=[(\"preprocessor\", ct), (\"classifier\", model)])\n",
    "        # Fit model on training data\n",
    "        pipeline.fit(X_combined, y_combined)\n",
    "        # Evaluate fit\n",
    "        y_predicted = pipeline.predict(X_original)\n",
    "        acc = accuracy_score(y_original, y_predicted)\n",
    "        f1_value = f1_score(y_original, y_predicted)\n",
    "        report = classification_report(y_original, y_predicted, output_dict=True)\n",
    "\n",
    "        results[model_names[i]] = {\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_score\": f1_value,\n",
    "            \"classification_report\": report\n",
    "        }\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-28T13:06:53.932147Z"
    }
   },
   "outputs": [],
   "source": [
    "## Do some testing on performing various degrees of incorrect data discovery.\n",
    "##\n",
    "\n",
    "y_combined = ds_combined['salary']\n",
    "X_combined = ds_combined.drop(columns=['salary'])\n",
    "\n",
    "# Initialise models\n",
    "random_forest = RandomForestClassifier()\n",
    "log_regr = LogisticRegression(max_iter=10000)\n",
    "svm = LinearSVC(dual=False, max_iter=10000)\n",
    "grad_boost = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n",
    "\n",
    "# Call method to train and evaluate\n",
    "models = [random_forest, log_regr, svm, grad_boost]\n",
    "model_names = [\"random_forest\", \"logistic_regression\", \"SVM\", \"Gradient Booster\"]\n",
    "results = train_test_models(X_combined, y_combined, X, y, models, model_names, ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-28T13:06:53.933155Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-28T13:06:53.934155Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussions\n",
    "\n",
    "1)  Discuss the different effects of the results of the data discovery results on various downstream tasks in the machine learning workflow.\n",
    "    As stated previously, this is effecting the data loading portion of the ML workflow.\n",
    "\n",
    "2)  Discuss also what aspects need to be considered when performing data discovery and evaluating the results of data discovery.\n",
    "\n",
    "Max. 400 words\n",
    "\n",
    "The perturbations that performed for part 1 and 2 are all data quality issues.\n",
    "\n",
    "3) Discuss on the effects of data quality and how you may attempt to identify and solve these issues?\n",
    "\n",
    "<!-- For the set of considerations that you have outlined for the choice of data discovery methods, choose one and identify under this new constraint, how would you identify and resolve this problem? -->\n",
    "\n",
    "Max 400 words\n",
    "\n",
    "There are some data quality issues are solved in part 1. The first issue is missing values which are represented by question mark or null value in columns like workclass, occupation and native-country. They could be treated as valid data which lead to incorrect conclusions. To deal with the missing values, the first approach is finding the instances contain missing values and remove them. Another approach is imputation which replaces missing values with estimated value. The second issue is the target label is inconsistent in column salary. It will confuse models and lead to wrong predictions. We use binary data (0,1) to indicate whether salary it exceeds 50K. Another issue is the data set contains redundant feature: education and education-num which may not provide additional value to models and lead to multicollinearity. To address this problem, the column named education was removed from the dataset. Some of the numerical features have skewed distributions like age, fnlwgt, capital-gain, capital-loss, hours-per-week. Due to these features may significantly influence the performance of models, we decided to apply normalization to reduce the impact.\n",
    "\n",
    "Apart from that, feature named native country contains 41 different categories, but most of instances in the dataset are from the United States and it does not show a relationship with the salary. Since this feature may act as noise, it was removed from the datasets. In addition, the chi-square and correlation tests were applied to salary and race. The results show race has a weak correlation with salary which may not help improve the performance, so the feature named race was also removed.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
